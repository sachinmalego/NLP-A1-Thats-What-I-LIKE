{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Preparation and Training\n",
    "\n",
    "### Objective:\n",
    "Build upon the code discussed in class to enhance understanding and implementation of Word2Vec and GloVe algorithms. The task emphasizes creating and modifying these algorithms without relying on pre-built solutions from the internet.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Read and Understand:\n",
    "- **Word2Vec Paper**: Study the foundational concepts and techniques outlined in the original Word2Vec paper.  \n",
    "- **GloVe Paper**: Comprehend the methodology and innovations introduced in the GloVe paper.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Code Modifications:\n",
    "#### a. Modify the Word2Vec (with and without negative sampling) and GloVe algorithms as discussed in the lab lecture.  \n",
    "- **Implementation Details**:\n",
    "  - Use a real-world corpus for training, such as categorizing news data from the **nltk dataset**.\n",
    "  - Source the dataset from reputable public databases or repositories and include proper citations in the documentation.\n",
    "  \n",
    "#### b. Create a Function for Dynamic Window Size Modification:\n",
    "- Develop a function to enable the dynamic adjustment of the window size during training.\n",
    "- **Default Window Size**: Set the default window size to 2.\n",
    "\n",
    "---\n",
    "\n",
    "## Additional Notes:\n",
    "- **Documentation**: Ensure that all dataset sources and citations are included in the documentation to maintain academic integrity.\n",
    "- **Evaluation**: Implement and validate the modifications on the selected corpus to verify the functionality of the updated algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data - Corpus and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-18 22:53:27.259 python[14899:607162] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-01-18 22:53:27.259 python[14899:607162] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "#download nltk corpus\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment we are asked to use a real world corpus from the nltk. Hence for this assignment I am using the brown corpus with the category of news as suggested in the instruction. \n",
    "\n",
    "#### **BROWN CORPUS (source: The below text has been copied from Wikipedia)**\n",
    "The Brown University Standard Corpus of Present-Day American English, better known as simply the Brown Corpus, is an electronic collection of text samples of American English, the first major structured corpus of varied genres. This corpus first set the bar for the scientific study of the frequency and distribution of word categories in everyday language use. Compiled by Henry Kučera and W. Nelson Francis at Brown University, in Rhode Island, it is a general language corpus containing 500 samples of English, totaling roughly one million words, compiled from works published in the United States in 1961.\n",
    "\n",
    "**Manual of Brown Corpus**: http://clu.uni.no/icame/manuals/  \n",
    "**NLTK Corpora (12. Brown Corpus)**: https://www.nltk.org/nltk_data/  \n",
    "id: brown; size: 3314357; author: W. N. Francis and H. Kucera; copyright: ; license: May be used for non-commercial purposes.;  \n",
    "**Download Brown Corpus**: https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/brown.zip  \n",
    "**Source of Brown Corpus**: http://www.hit.uib.no/icame/brown/bcm.html\n",
    "\n",
    "**Categorizing and Tagging Words in nltk**: https://www.nltk.org/book/ch05.html\n",
    "\n",
    "##### Importing the corpus and tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "corpus = brown.sents(categories='news')\n",
    "\n",
    "#Here from the corpus we are selecting the first 1000000 words\n",
    "corpus = corpus[:1000000]\n",
    "\n",
    "#lowercase the word in corpus\n",
    "corpus = [[word.lower() for word in sent] for sent in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numeralization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find unique words\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "#assign unique integer\n",
    "vocabs = list(set(flatten(corpus))) #all the words we have in the system - <UNK>\n",
    "\n",
    "#append UNKNOWN token to vocabs\n",
    "vocabs.append('<UNK>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11852"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create handy mapping between integer and word\n",
    "word2index = {v:idx for idx, v in enumerate(vocabs)}\n",
    "word2index['dog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index2word = {v:k for k, v in word2index.items()}\n",
    "index2word[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare train data random batch function with window size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create pairs of center word, and outside word\n",
    "\n",
    "def random_batch(batch_size, corpus, window_size):\n",
    "\n",
    "    skipgrams = []\n",
    "\n",
    "    #loop each corpus\n",
    "    for doc in corpus:\n",
    "        #look from the 2nd word until second last word\n",
    "        for i in range(window_size, len(doc)-window_size):\n",
    "            #center word\n",
    "            center = word2index[doc[i]]\n",
    "            #outside words = 2 words\n",
    "            outside = []\n",
    "            for j in range(i-window_size, i+window_size+1):\n",
    "                outside.append(word2index[doc[j]])\n",
    "            #for each of these two outside words, we gonna append to a list\n",
    "            for each_out in outside:\n",
    "                skipgrams.append([center, each_out])\n",
    "                #center, outside1;   center, outside2\n",
    "                \n",
    "    random_index = np.random.choice(range(len(skipgrams)), batch_size, replace=False)\n",
    "    \n",
    "    inputs, labels = [], []\n",
    "    for index in random_index:\n",
    "        inputs.append([skipgrams[index][0]])\n",
    "        labels.append([skipgrams[index][1]])\n",
    "        \n",
    "    return np.array(inputs), np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2. Model Comparison and Analysis\n",
    "\n",
    "#### 1) Compare Skip-gram, Skip-gram negative sampling, GloVe models on training loss, training time. (1points)\n",
    "#### 2) Use Word analogies dataset 3 to calucalte between syntactic and semantic accuracy, similar to the methods in the Word2Vec and GloVe paper. (1 points)\n",
    "- **Note** : using only capital-common-countries for semantic and past-tense for syntactic.\n",
    "- **Note** : Do not be surprised if you achieve 0% accuracy in these experiments, as this may be due to the limitations of our corpus. If you are curious, you can try the same experiments with a pre-trained GloVe model from the Gensim library for a comparison.\n",
    "\n",
    "#### 3) Use the similarity dataset4to find the correlation between your models’ dot product and the provided similarity metrics. (from scipy.stats import spearmanr) Assess if your embeddings correlate with human judgment. (1 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec (with and without negative sampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unigram distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100554"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count\n",
    "word_count = Counter(flatten(corpus))\n",
    "word_count\n",
    "\n",
    "#get the total number of words\n",
    "num_total_words = sum([c for w, c in word_count.items()])\n",
    "num_total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_table = []\n",
    "\n",
    "for v in vocabs:\n",
    "    uw = word_count[v] / num_total_words\n",
    "    uw_alpha = int((uw ** 0.75) / z)\n",
    "    unigram_table.extend([v] * uw_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, word2index):\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], seq))\n",
    "    return torch.LongTensor(idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def negative_sampling(targets, unigram_table, k):\n",
    "    batch_size = targets.shape[0]\n",
    "    neg_samples = []\n",
    "    for i in range(batch_size):  #(1, k)\n",
    "        target_index = targets[i].item()\n",
    "        nsample      = []\n",
    "        while (len(nsample) < k):\n",
    "            neg = random.choice(unigram_table)\n",
    "            if word2index[neg] == target_index:\n",
    "                continue\n",
    "            nsample.append(neg)\n",
    "        neg_samples.append(prepare_sequence(nsample, word2index).reshape(1, -1))\n",
    "        \n",
    "    return torch.cat(neg_samples) #batch_size, k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec (without negative sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Skipgram(nn.Module):\n",
    "    \n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(Skipgram, self).__init__()\n",
    "        self.embedding_center  = nn.Embedding(voc_size, emb_size)\n",
    "        self.embedding_outside = nn.Embedding(voc_size, emb_size)\n",
    "    \n",
    "    def forward(self, center, outside, all_vocabs):\n",
    "        center_embedding     = self.embedding_center(center)  #(batch_size, 1, emb_size)\n",
    "        outside_embedding    = self.embedding_center(outside) #(batch_size, 1, emb_size)\n",
    "        all_vocabs_embedding = self.embedding_center(all_vocabs) #(batch_size, voc_size, emb_size)\n",
    "        \n",
    "        top_term = torch.exp(outside_embedding.bmm(center_embedding.transpose(1, 2)).squeeze(2))\n",
    "        #batch_size, 1, emb_size) @ (batch_size, emb_size, 1) = (batch_size, 1, 1) = (batch_size, 1) \n",
    "\n",
    "        lower_term = all_vocabs_embedding.bmm(center_embedding.transpose(1, 2)).squeeze(2)\n",
    "        #batch_size, voc_size, emb_size) @ (batch_size, emb_size, 1) = (batch_size, voc_size, 1) = (batch_size, voc_size) \n",
    "        \n",
    "        lower_term_sum = torch.sum(torch.exp(lower_term), 1)  #(batch_size, 1)\n",
    "        \n",
    "        loss = -torch.mean(torch.log(top_term / lower_term_sum))  #scalar\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec (with negative sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipgramNeg(nn.Module):\n",
    "    \n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(SkipgramNeg, self).__init__()\n",
    "        self.embedding_center  = nn.Embedding(voc_size, emb_size)\n",
    "        self.embedding_outside = nn.Embedding(voc_size, emb_size)\n",
    "        self.logsigmoid        = nn.LogSigmoid()\n",
    "    \n",
    "    def forward(self, center, outside, negative):\n",
    "        #center, outside:  (bs, 1)\n",
    "        #negative       :  (bs, k)\n",
    "        \n",
    "        center_embed   = self.embedding_center(center) #(bs, 1, emb_size)\n",
    "        outside_embed  = self.embedding_outside(outside) #(bs, 1, emb_size)\n",
    "        negative_embed = self.embedding_outside(negative) #(bs, k, emb_size)\n",
    "        \n",
    "        uovc           = outside_embed.bmm(center_embed.transpose(1, 2)).squeeze(2) #(bs, 1)\n",
    "        ukvc           = -negative_embed.bmm(center_embed.transpose(1, 2)).squeeze(2) #(bs, k)\n",
    "        ukvc_sum       = torch.sum(ukvc, 1).reshape(-1, 1) #(bs, 1)\n",
    "        \n",
    "        loss           = self.logsigmoid(uovc) + self.logsigmoid(ukvc_sum)\n",
    "        \n",
    "        return -torch.mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since, I am doing this in MacBook hence I am using the MAC 'mps' | 'cpu' whichever  is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set device (MPS if available)\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "emb_size = 2\n",
    "window_size = 2 #default window_size\n",
    "voc_size = len(vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,     1,     2,  ..., 13110, 13111, 13112],\n",
       "        [    0,     1,     2,  ..., 13110, 13111, 13112]], device='mps:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prepare all vocabs\n",
    "all_vocabs = prepare_sequence(list(vocabs), word2index).expand(batch_size, voc_size).to(device)\n",
    "all_vocabs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Skipgram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_skipgram = Skipgram(voc_size, emb_size).to(device)\n",
    "optimizer = optim.Adam(model_skipgram.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1000 | Loss: 9.694761\n",
      "Epoch   2000 | Loss: 8.396974\n",
      "Epoch   3000 | Loss: 9.138665\n",
      "Epoch   4000 | Loss: 9.502065\n",
      "Epoch   5000 | Loss: 9.731103\n",
      "Epoch   6000 | Loss: 9.530214\n",
      "Epoch   7000 | Loss: 9.079416\n",
      "Epoch   8000 | Loss: 9.076571\n",
      "Epoch   9000 | Loss: 9.572384\n",
      "Epoch  10000 | Loss: 9.767000\n",
      "Training time: 642.6175730228424\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10000\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    #getbatch\n",
    "    input_batch, label_batch = random_batch(batch_size, corpus, window_size)\n",
    "    input_tensor = torch.LongTensor(input_batch).to(device)\n",
    "    label_tensor = torch.LongTensor(label_batch).to(device)\n",
    "\n",
    "    #predict\n",
    "    loss = model_skipgram(input_tensor, label_tensor, all_vocabs)\n",
    "\n",
    "    #backprogate\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    #update alpha\n",
    "    optimizer.step()\n",
    "\n",
    "    #print the loss\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print(f\"Epoch {epoch+1:6.0f} | Loss: {loss:2.6f}\")\n",
    "\n",
    "print(f\"Training time: {time.time()-start_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Skipgram Negative "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_skipgram_neg = SkipgramNeg(voc_size, emb_size).to(device)\n",
    "optimizer = optim.Adam(model_skipgram_neg.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1000 | Loss: 2.038834\n",
      "Epoch   2000 | Loss: 3.213029\n",
      "Epoch   3000 | Loss: 0.510042\n",
      "Epoch   4000 | Loss: 1.598014\n",
      "Epoch   5000 | Loss: 1.543426\n",
      "Epoch   6000 | Loss: 1.063593\n",
      "Epoch   7000 | Loss: 2.304073\n",
      "Epoch   8000 | Loss: 2.273893\n",
      "Epoch   9000 | Loss: 2.072499\n",
      "Epoch  10000 | Loss: 1.027903\n",
      "Training time: 651.9084329605103\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10000\n",
    "k = 5\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    #get batch\n",
    "    input_batch, label_batch = random_batch(batch_size, corpus, window_size)\n",
    "    input_tensor = torch.LongTensor(input_batch).to(device)\n",
    "    label_tensor = torch.LongTensor(label_batch).to(device)\n",
    "\n",
    "    #predict\n",
    "    neg_samples = negative_sampling(label_tensor, unigram_table, k).to(device)\n",
    "    loss = model_skipgram_neg(input_tensor, label_tensor, neg_samples)\n",
    "    \n",
    "    #backprogate\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    #update alpha\n",
    "    optimizer.step()\n",
    "    \n",
    "    #print the loss\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print(f\"Epoch {epoch+1:6.0f} | Loss: {loss:2.6f}\")\n",
    "\n",
    "print(f\"Training time: {time.time()-start_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe (Scratch)\n",
    "\n",
    "Let's work on implementation of GloVE.\n",
    "\n",
    "#### Build Co-occurence Matrix X\n",
    "Here, we need to count the co-occurence of two words given some window size.  We gonna use window size of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_i = Counter(flatten(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_grams = []\n",
    "\n",
    "for doc in corpus:\n",
    "    for i in range(1, len(doc)-window_size):\n",
    "        center = doc[i]\n",
    "        outside = [doc[i-window_size], doc[i-1], doc[i+1], doc[i+window_size]]\n",
    "        for each_out in outside:\n",
    "            skip_grams.append((center, each_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ik_skipgrams = Counter(skip_grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weighting function\n",
    "GloVe includes a weighting function to scale down too frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighting(w_i, w_j, X_ik):\n",
    "    \n",
    "    #check whether the co-occurences between w_i and w_j is available\n",
    "    try:\n",
    "        x_ij = X_ik[(w_i, w_j)]\n",
    "        #if not exist, then set to 1 \"laplace smoothing\"\n",
    "    except:\n",
    "        x_ij = 1\n",
    "        \n",
    "    #set xmax\n",
    "    x_max = 100\n",
    "    #set alpha\n",
    "    alpha = 0.75\n",
    "    \n",
    "    #if co-ocurrence does not exceeed xmax, then just multiply with some alpha\n",
    "    if x_ij < x_max:\n",
    "        result = (x_ij / x_max)**alpha\n",
    "    #otherwise, set to 1\n",
    "    else:\n",
    "        result = 1\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations_with_replacement\n",
    "\n",
    "X_ik = {} #keeping the co-occurences\n",
    "weighting_dic = {} #already scale the co-occurences using the weighting function\n",
    "\n",
    "for bigram in combinations_with_replacement(vocabs, 2):\n",
    "    if X_ik_skipgrams.get(bigram):  #if the pair exists in our corpus\n",
    "        co = X_ik_skipgrams[bigram]\n",
    "        X_ik[bigram] = co + 1 #for stability\n",
    "        X_ik[(bigram[1], bigram[0])] = co + 1 #basically apple, banana = banana, apple\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    weighting_dic[bigram] = weighting(bigram[0], bigram[1], X_ik)\n",
    "    weighting_dic[(bigram[1], bigram[0])] = weighting(bigram[1], bigram[0], X_ik)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(batch_size, word_sequence, skip_grams, X_ik, weighting_dic):\n",
    "    \n",
    "    random_inputs, random_labels, random_coocs, random_weightings = [], [], [], []\n",
    "    \n",
    "    #convert our skipgrams to id\n",
    "    skip_grams_id = [(word2index[skip_gram[0]], word2index[skip_gram[1]]) for skip_gram in skip_grams]\n",
    "    \n",
    "    #randomly choose indexes based on batch size\n",
    "    random_index = np.random.choice(range(len(skip_grams_id)), batch_size, replace=False)\n",
    "    \n",
    "    #get the random input and labels\n",
    "    for index in random_index:\n",
    "        random_inputs.append([skip_grams_id[index][0]])\n",
    "        random_labels.append([skip_grams_id[index][1]])\n",
    "        #coocs\n",
    "        pair = skip_grams[index] #e.g., ('banana', 'fruit')\n",
    "        try:\n",
    "            cooc = X_ik[pair]\n",
    "        except:\n",
    "            cooc = 1\n",
    "        random_coocs.append([math.log(cooc)])\n",
    "    \n",
    "        #weightings\n",
    "        weighting = weighting_dic[pair]\n",
    "        random_weightings.append([weighting])\n",
    "        \n",
    "    return np.array(random_inputs), np.array(random_labels), np.array(random_coocs), np.array(random_weightings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Glove(nn.Module):\n",
    "    \n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(Glove, self).__init__()\n",
    "        self.embedding_center  = nn.Embedding(voc_size, emb_size)\n",
    "        self.embedding_outside = nn.Embedding(voc_size, emb_size)\n",
    "        \n",
    "        self.center_bias       = nn.Embedding(voc_size, 1) \n",
    "        self.outside_bias      = nn.Embedding(voc_size, 1)\n",
    "    \n",
    "    def forward(self, center, outside, coocs, weighting):\n",
    "        center_embeds  = self.embedding_center(center) #(batch_size, 1, emb_size)\n",
    "        outside_embeds = self.embedding_outside(outside) #(batch_size, 1, emb_size)\n",
    "        \n",
    "        center_bias    = self.center_bias(center).squeeze(1)\n",
    "        target_bias    = self.outside_bias(outside).squeeze(1)\n",
    "        \n",
    "        inner_product  = outside_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
    "        #(batch_size, 1, emb_size) @ (batch_size, emb_size, 1) = (batch_size, 1, 1) = (batch_size, 1)\n",
    "        \n",
    "        loss = weighting * torch.pow(inner_product + center_bias + target_bias - coocs, 2)\n",
    "        \n",
    "        return torch.sum(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size     = 10 # mini-batch size\n",
    "embedding_size = 2 #so we can later plot\n",
    "model_glove_scratch = Glove(voc_size, embedding_size).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_glove_scratch.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1000 | cost: 35.283993 | time: 0m 0s\n",
      "Epoch: 2000 | cost: 21.909081 | time: 0m 0s\n",
      "Epoch: 3000 | cost: 17.584835 | time: 0m 0s\n",
      "Epoch: 4000 | cost: 13.305610 | time: 0m 0s\n",
      "Epoch: 5000 | cost: 4.621394 | time: 0m 0s\n",
      "Epoch: 6000 | cost: 4.300721 | time: 0m 0s\n",
      "Epoch: 7000 | cost: 11.993736 | time: 0m 0s\n",
      "Epoch: 8000 | cost: 6.335316 | time: 0m 0s\n",
      "Epoch: 9000 | cost: 5.531918 | time: 0m 0s\n",
      "Epoch: 10000 | cost: 0.672632 | time: 0m 0s\n",
      "Total Training time: 1214.7566788196564\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10000\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    input_batch, target_batch, cooc_batch, weighting_batch = random_batch(batch_size, corpus, skip_grams, X_ik, weighting_dic)\n",
    "    input_batch  = torch.LongTensor(input_batch).to(device)         #[batch_size, 1]\n",
    "    target_batch = torch.LongTensor(target_batch).to(device)        #[batch_size, 1]\n",
    "    cooc_batch   = torch.FloatTensor(cooc_batch).to(device)         #[batch_size, 1]\n",
    "    weighting_batch = torch.FloatTensor(weighting_batch).to(device) #[batch_size, 1]\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss = model_glove_scratch(input_batch, target_batch, cooc_batch, weighting_batch)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start, end)\n",
    "\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print(f\"Epoch: {epoch + 1} | cost: {loss:.6f} | time: {epoch_mins}m {epoch_secs}s\")\n",
    "\n",
    "print(f\"Total Training time: {time.time()-start_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe (Gensim)\n",
    "For looking at word vectors, we'll use **Gensim**. **Gensim** isn't really a deep learning package. It's a package for for word and text similarity modeling, which started with (LDA-style) topic models and grew into SVD and neural word representations. But its efficient and scalable, and quite widely used.   We gonna use **GloVe** embeddings, downloaded at [the Glove page](https://nlp.stanford.edu/projects/glove/). They're inside [this zip file](https://nlp.stanford.edu/data/glove.6B.zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "#you have to put this file in some python/gensim directory; just run it and it will inform where to put....\n",
    "glove_file = datapath(os.path.abspath('word_test/glove.6B.100d.txt'))  #search on the google\n",
    "model_glove_gensim = KeyedVectors.load_word2vec_format(glove_file, binary=False, no_header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Word analogies dataset 3 to calculate between **syntactic and semantic** accuracy, similar to the methods in the Word2Vec and GloVe paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_embeddings(model, vocabs):\n",
    "    embeds = {}\n",
    "\n",
    "    device = torch.device(\"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    for word in vocabs:\n",
    "        try:\n",
    "            index = word2index[word]\n",
    "        except:\n",
    "            index = word2index['<UNK>']\n",
    " \n",
    "        word_idx = torch.LongTensor([word2index[word]])\n",
    "        \n",
    "        embed_c = model.embedding_center(word_idx)\n",
    "        embed_o = model.embedding_outside(word_idx)\n",
    "        embed   = (embed_c + embed_o) / 2\n",
    "        embed = embed[0][0].item(), embed[0][1].item()\n",
    "        embeds[word] = np.array(embed)\n",
    "    \n",
    "    return embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embed(embeddings, word):\n",
    "    try:\n",
    "        index = word2index[word]\n",
    "    except:\n",
    "        word = '<UNK>'\n",
    "    \n",
    "    return embeddings[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the embeddings from each of our model\n",
    "emb_skipgram = comp_embeddings(model_skipgram, vocabs)\n",
    "emb_skipgram_neg = comp_embeddings(model_skipgram_neg, vocabs)\n",
    "emb_glove_scratch = comp_embeddings(model_glove_scratch, vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved emb_skipgram embeddings to app/pickle/emb_skipgram.pickle\n",
      "Saved emb_skipgram_neg embeddings to app/pickle/emb_skipgram_neg.pickle\n",
      "Saved emb_glove_scratch embeddings to app/pickle/emb_glove_scratch.pickle\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "# Define embeddings dictionary\n",
    "embeds_dict = {\n",
    "    \"emb_skipgram\": emb_skipgram,\n",
    "    \"emb_skipgram_neg\": emb_skipgram_neg,\n",
    "    \"emb_glove_scratch\": emb_glove_scratch\n",
    "}\n",
    "\n",
    "# Define the directory for saving embeddings\n",
    "output_dir = Path(\"app/pickle\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)  # Ensure directory exists\n",
    "\n",
    "# Save each embedding to a separate file\n",
    "for name, embed in embeds_dict.items():\n",
    "    file_path = output_dir / f\"{name}.pickle\"\n",
    "    try:\n",
    "        with file_path.open(\"wb\") as f:\n",
    "            pickle.dump(embed, f)\n",
    "        print(f\"Saved {name} embeddings to {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving {name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipgram: [-1.12307346  0.43248087]\n",
      "Skipgram NEG: [-0.31465432 -0.21734357]\n",
      "GloVe: [-0.62375081 -0.61236209]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Skipgram: {get_embed(emb_skipgram, 'greece')}\")\n",
    "print(f\"Skipgram NEG: {get_embed(emb_skipgram_neg, 'greece')}\")\n",
    "print(f\"GloVe: {get_embed(emb_glove_scratch, 'greece')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read analogy of \"word-test.v1.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_analogy_dataset(file_path):\n",
    "    from collections import defaultdict\n",
    "\n",
    "    analogy_dict = defaultdict(list)  # To store analogies grouped by categories\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, \"r\") as f:\n",
    "            lines = f.read().splitlines()\n",
    "        \n",
    "        current_category = None\n",
    "        for line in lines:\n",
    "            line = line.strip()  # Remove surrounding whitespace\n",
    "            if not line:  # Skip empty lines\n",
    "                continue\n",
    "            \n",
    "            if line.startswith(': '):  # Category line\n",
    "                current_category = line[2:].strip()\n",
    "            elif current_category:  # Analogy line\n",
    "                analogy_dict[current_category].append(line.split())\n",
    "        \n",
    "        return dict(analogy_dict)\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File {file_path} not found.\")\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: capital-common-countries\n",
      "Analogies: [['Athens', 'Greece', 'Baghdad', 'Iraq'], ['Athens', 'Greece', 'Bangkok', 'Thailand'], ['Athens', 'Greece', 'Beijing', 'China'], ['Athens', 'Greece', 'Berlin', 'Germany'], ['Athens', 'Greece', 'Bern', 'Switzerland']]\n"
     ]
    }
   ],
   "source": [
    "file_path = \"word_test/word-test.v1.txt\"\n",
    "analogy_dict = read_analogy_dataset(file_path)\n",
    "\n",
    "#Print first category and its analogies\n",
    "if analogy_dict:\n",
    "    first_category = next(iter(analogy_dict))\n",
    "    print(f\"Category: {first_category}\")\n",
    "    print(\"Analogies:\", analogy_dict[first_category][:5])  # Show first 5 analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Athens', 'Greece', 'Baghdad', 'Iraq'],\n",
       " ['Athens', 'Greece', 'Bangkok', 'Thailand'],\n",
       " ['Athens', 'Greece', 'Beijing', 'China'],\n",
       " ['Athens', 'Greece', 'Berlin', 'Germany'],\n",
       " ['Athens', 'Greece', 'Bern', 'Switzerland']]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the 'capital-common-countries' section\n",
    "capital = analogy_dict.get('capital-common-countries', [])\n",
    "\n",
    "# To print the first 5 analogies from the section\n",
    "capital[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['dancing', 'danced', 'decreasing', 'decreased'],\n",
       " ['dancing', 'danced', 'describing', 'described'],\n",
       " ['dancing', 'danced', 'enhancing', 'enhanced'],\n",
       " ['dancing', 'danced', 'falling', 'fell'],\n",
       " ['dancing', 'danced', 'feeding', 'fed']]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the 'gram7-past-tense' section from analogy_dict\n",
    "past_tense = analogy_dict.get('gram7-past-tense', [])\n",
    "\n",
    "# Display the first 5 analogies\n",
    "past_tense[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Athens', 'Greece', 'Bangkok', 'Thailand']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "capital[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding: emb_skipgram\n",
      "y_pred: [-1.12307346  0.43248087]\n",
      "=================================\n",
      "Embedding: emb_skipgram_neg\n",
      "y_pred: [-0.31465432 -0.21734357]\n",
      "=================================\n",
      "Embedding: emb_glove_scratch\n",
      "y_pred: [-0.62375081 -0.61236209]\n",
      "=================================\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "embeddings = ['emb_skipgram', 'emb_skipgram_neg', 'emb_glove_scratch']\n",
    "\n",
    "# Assuming the embeddings are stored as variables like `emb_skipgram`, `emb_skipgram_neg`, and 'emb_glove_scratch'.\n",
    "for embed_name in embeddings:\n",
    "    # Dynamically fetch the embedding variable using globals()\n",
    "    emb_w1 = get_embed(globals()[embed_name], capital[i][1].lower())\n",
    "    emb_w2 = get_embed(globals()[embed_name], capital[i][0].lower())\n",
    "    emb_w3 = get_embed(globals()[embed_name], capital[i][2].lower())\n",
    "\n",
    "    y_pred = emb_w1 - emb_w2 + emb_w3\n",
    "    print(f\"Embedding: {embed_name}\")\n",
    "    print(f\"y_pred: {y_pred}\")\n",
    "    print(\"=================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(A, B):\n",
    "    dot_product = np.dot(A, B)\n",
    "    norm_a = np.linalg.norm(A)\n",
    "    norm_b = np.linalg.norm(B)\n",
    "    similarity = dot_product / (norm_a * norm_b)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find the most similar word given the input vector\n",
    "def get_most_similar(vector, embeddings):\n",
    "    try:\n",
    "        words = list(embeddings.keys())\n",
    "    except:\n",
    "        words = list(embeddings.key_to_index.keys())\n",
    "    \n",
    "    # Precompute norms of all embeddings to avoid redundant calculations\n",
    "    norms = {word: np.linalg.norm(embedding) for word, embedding in embeddings.items()}\n",
    "    \n",
    "    # Calculate similarities and keep the word with the highest similarity\n",
    "    similarities = {}\n",
    "    for word, embedding in embeddings.items():\n",
    "        similarity = np.dot(vector, embedding) / (norms[word] * np.linalg.norm(vector))\n",
    "        similarities[word] = similarity\n",
    "    \n",
    "    return max(similarities, key=similarities.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find the most similar word given the input vector cosine_ranking\n",
    "def cosine_ranking(vector, embeddings):\n",
    "    try:\n",
    "        words = list(embeddings.keys())\n",
    "    except:\n",
    "        words = list(embeddings.key_to_index.keys())\n",
    "    \n",
    "    # Precompute norms of all embeddings to avoid redundant calculations\n",
    "    norms = {word: np.linalg.norm(embedding) for word, embedding in embeddings.items()}\n",
    "    \n",
    "    similarities = {}\n",
    "    for word, embedding in embeddings.items():\n",
    "        similarity = np.dot(vector, embedding) / (norms[word] * np.linalg.norm(vector))\n",
    "        similarities[word] = similarity\n",
    "    \n",
    "    # Return the dictionary sorted by similarity values in descending order\n",
    "    return dict(sorted(similarities.items(), key=lambda item: item[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find semantic and syntactic accuracy\n",
    "def find_accuracy(dataset, embeddings):\n",
    "    matched_count = 0\n",
    "\n",
    "    for data in dataset:\n",
    "        row = [word.lower() for word in data]\n",
    "        \n",
    "        try:\n",
    "            pred_y = get_embed(embeddings, row[1]) - get_embed(embeddings, row[0]) + get_embed(embeddings, row[2])\n",
    "            pred_word = get_most_similar(pred_y, embeddings)\n",
    "        except:\n",
    "            pred_word = embeddings.most_similar(positive=[row[1], row[2]], negative=[row[0]])[0][0]\n",
    "\n",
    "        if row[3] == pred_word:\n",
    "            matched_count += 1\n",
    "\n",
    "    return matched_count / len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Word2Vec Skipgram ==\n",
      "Semantic accuracy: 0.0\n",
      "Syntactic accuracy: 0.000641025641025641\n"
     ]
    }
   ],
   "source": [
    "skipgram_semantic_acc = find_accuracy(capital, emb_skipgram)\n",
    "skipgram_syntactic_acc = find_accuracy(past_tense, emb_skipgram)\n",
    "\n",
    "print(\"== Word2Vec Skipgram ==\")\n",
    "print(f\"Semantic accuracy: {skipgram_semantic_acc}\")\n",
    "print(f\"Syntactic accuracy: {skipgram_syntactic_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Word2Vec Skipgram (NEG) ==\n",
      "Semantic accuracy: 0.0\n",
      "Syntactic accuracy: 0.000641025641025641\n"
     ]
    }
   ],
   "source": [
    "skipgram_neg_semantic_acc = find_accuracy(capital, emb_skipgram_neg)\n",
    "skipgram_neg_syntactic_acc = find_accuracy(past_tense, emb_skipgram_neg)\n",
    "\n",
    "print(\"== Word2Vec Skipgram (NEG) ==\")\n",
    "print(f\"Semantic accuracy: {skipgram_neg_semantic_acc}\")\n",
    "print(f\"Syntactic accuracy: {skipgram_neg_syntactic_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== GloVe (Scratch) ==\n",
      "Semantic accuracy: 0.0\n",
      "Syntactic accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "glove_scratch_semantic_acc = find_accuracy(capital, emb_glove_scratch)\n",
    "glove_scratch_syntactic_acc = find_accuracy(past_tense, emb_glove_scratch)\n",
    "\n",
    "print(\"== GloVe (Scratch) ==\")\n",
    "print(f\"Semantic accuracy: {glove_scratch_semantic_acc}\")\n",
    "print(f\"Syntactic accuracy: {glove_scratch_syntactic_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== GloVe (Gensim) ==\n",
      "Semantic accuracy: 0.9387351778656127\n",
      "Syntactic accuracy: 0.5544871794871795\n"
     ]
    }
   ],
   "source": [
    "glove_gensim_semantic_acc = find_accuracy(capital, model_glove_gensim)\n",
    "glove_gensim_syntactic_acc = find_accuracy(past_tense, model_glove_gensim)\n",
    "\n",
    "print(\"== GloVe (Gensim) ==\")\n",
    "print(f\"Semantic accuracy: {glove_gensim_semantic_acc}\")\n",
    "print(f\"Syntactic accuracy: {glove_gensim_syntactic_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarity Correlation\n",
    "Use the similarity dataset 4 to find the correlation between your models’ dot product and the provided similarity metrics. (from scipy.stats import spearmanr) Assess if your embeddings correlate with human judgment. (1 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_1</th>\n",
       "      <th>word_2</th>\n",
       "      <th>similarities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tiger</td>\n",
       "      <td>cat</td>\n",
       "      <td>7.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tiger</td>\n",
       "      <td>tiger</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>plane</td>\n",
       "      <td>car</td>\n",
       "      <td>5.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train</td>\n",
       "      <td>car</td>\n",
       "      <td>6.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>television</td>\n",
       "      <td>radio</td>\n",
       "      <td>6.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>rooster</td>\n",
       "      <td>voyage</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>noon</td>\n",
       "      <td>string</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>chord</td>\n",
       "      <td>smile</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>professor</td>\n",
       "      <td>cucumber</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>king</td>\n",
       "      <td>cabbage</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>203 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         word_1    word_2  similarities\n",
       "0         tiger       cat          7.35\n",
       "1         tiger     tiger         10.00\n",
       "2         plane       car          5.77\n",
       "3         train       car          6.31\n",
       "4    television     radio          6.77\n",
       "..          ...       ...           ...\n",
       "198     rooster    voyage          0.62\n",
       "199        noon    string          0.54\n",
       "200       chord     smile          0.54\n",
       "201   professor  cucumber          0.31\n",
       "202        king   cabbage          0.23\n",
       "\n",
       "[203 rows x 3 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the similarity dataset\n",
    "word_sim = pd.read_csv('word_test/wordsim_similarity_goldstandard.txt', sep = \"\\t\", header = None, names = ['word_1', 'word_2', 'similarities'])\n",
    "word_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to map column names to their respective embedding models or methods\n",
    "embedding_models = {\n",
    "    'skipgram_dot_product': emb_skipgram,\n",
    "    'skipgram_neg_dot_product': emb_skipgram_neg,\n",
    "    'glove_scratch_dot_product': emb_glove_scratch,\n",
    "    'glove_gensim_dot_product': model_glove_gensim,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute dot products\n",
    "def compute_dot_product(embedding_model, word1, word2):\n",
    "    if isinstance(embedding_model, dict):  # For custom embeddings like skipgram or glove\n",
    "        return np.dot(\n",
    "            get_embed(embedding_model, word1.lower()),\n",
    "            get_embed(embedding_model, word2.lower())\n",
    "        )\n",
    "    elif hasattr(embedding_model, '__getitem__'):  # For Gensim models or similar\n",
    "        return np.dot(embedding_model[word1.lower()], embedding_model[word2.lower()])\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported embedding model type.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each embedding type and compute the dot products\n",
    "for column_name, model in embedding_models.items():\n",
    "    word_sim[column_name] = word_sim.apply(\n",
    "        lambda row: compute_dot_product(model, row['word_1'], row['word_2']), axis=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_1</th>\n",
       "      <th>word_2</th>\n",
       "      <th>similarities</th>\n",
       "      <th>skipgram_dot_product</th>\n",
       "      <th>skipgram_neg_dot_product</th>\n",
       "      <th>glove_scratch_dot_product</th>\n",
       "      <th>glove_gensim_dot_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tiger</td>\n",
       "      <td>cat</td>\n",
       "      <td>7.35</td>\n",
       "      <td>0.374398</td>\n",
       "      <td>0.448477</td>\n",
       "      <td>0.485633</td>\n",
       "      <td>15.629377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tiger</td>\n",
       "      <td>tiger</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.374398</td>\n",
       "      <td>0.448477</td>\n",
       "      <td>0.485633</td>\n",
       "      <td>32.800144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>plane</td>\n",
       "      <td>car</td>\n",
       "      <td>5.77</td>\n",
       "      <td>0.171009</td>\n",
       "      <td>-1.264918</td>\n",
       "      <td>-0.161668</td>\n",
       "      <td>24.047298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train</td>\n",
       "      <td>car</td>\n",
       "      <td>6.31</td>\n",
       "      <td>-0.100144</td>\n",
       "      <td>-0.349281</td>\n",
       "      <td>0.635021</td>\n",
       "      <td>25.472923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>television</td>\n",
       "      <td>radio</td>\n",
       "      <td>6.77</td>\n",
       "      <td>-0.623952</td>\n",
       "      <td>-0.150336</td>\n",
       "      <td>0.366628</td>\n",
       "      <td>34.689987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>rooster</td>\n",
       "      <td>voyage</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.374398</td>\n",
       "      <td>0.448477</td>\n",
       "      <td>0.485633</td>\n",
       "      <td>1.683646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>noon</td>\n",
       "      <td>string</td>\n",
       "      <td>0.54</td>\n",
       "      <td>-0.323274</td>\n",
       "      <td>-0.993186</td>\n",
       "      <td>-0.530243</td>\n",
       "      <td>1.070593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>chord</td>\n",
       "      <td>smile</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.068867</td>\n",
       "      <td>0.126992</td>\n",
       "      <td>0.376086</td>\n",
       "      <td>6.762520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>professor</td>\n",
       "      <td>cucumber</td>\n",
       "      <td>0.31</td>\n",
       "      <td>-0.489723</td>\n",
       "      <td>-0.545512</td>\n",
       "      <td>-0.588388</td>\n",
       "      <td>-0.230552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>king</td>\n",
       "      <td>cabbage</td>\n",
       "      <td>0.23</td>\n",
       "      <td>-0.174933</td>\n",
       "      <td>0.693191</td>\n",
       "      <td>-1.000759</td>\n",
       "      <td>1.400288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>203 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         word_1    word_2  similarities  skipgram_dot_product  \\\n",
       "0         tiger       cat          7.35              0.374398   \n",
       "1         tiger     tiger         10.00              0.374398   \n",
       "2         plane       car          5.77              0.171009   \n",
       "3         train       car          6.31             -0.100144   \n",
       "4    television     radio          6.77             -0.623952   \n",
       "..          ...       ...           ...                   ...   \n",
       "198     rooster    voyage          0.62              0.374398   \n",
       "199        noon    string          0.54             -0.323274   \n",
       "200       chord     smile          0.54              0.068867   \n",
       "201   professor  cucumber          0.31             -0.489723   \n",
       "202        king   cabbage          0.23             -0.174933   \n",
       "\n",
       "     skipgram_neg_dot_product  glove_scratch_dot_product  \\\n",
       "0                    0.448477                   0.485633   \n",
       "1                    0.448477                   0.485633   \n",
       "2                   -1.264918                  -0.161668   \n",
       "3                   -0.349281                   0.635021   \n",
       "4                   -0.150336                   0.366628   \n",
       "..                        ...                        ...   \n",
       "198                  0.448477                   0.485633   \n",
       "199                 -0.993186                  -0.530243   \n",
       "200                  0.126992                   0.376086   \n",
       "201                 -0.545512                  -0.588388   \n",
       "202                  0.693191                  -1.000759   \n",
       "\n",
       "     glove_gensim_dot_product  \n",
       "0                   15.629377  \n",
       "1                   32.800144  \n",
       "2                   24.047298  \n",
       "3                   25.472923  \n",
       "4                   34.689987  \n",
       "..                        ...  \n",
       "198                  1.683646  \n",
       "199                  1.070593  \n",
       "200                  6.762520  \n",
       "201                 -0.230552  \n",
       "202                  1.400288  \n",
       "\n",
       "[203 rows x 7 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "# List of embedding similarity columns and their names\n",
    "embedding_columns = {\n",
    "    'Word2Vec Skipgram': 'skipgram_dot_product',\n",
    "    'Word2Vec Skipgram (NEG)': 'skipgram_neg_dot_product',\n",
    "    'GloVe (Scratch)': 'glove_scratch_dot_product',\n",
    "    'GloVe (Gensim)': 'glove_gensim_dot_product',\n",
    "    'Y_true': 'similarities'  # Adding for reference comparison\n",
    "}\n",
    "\n",
    "# Convert the wordsim similarities to numpy for efficiency\n",
    "wordsim_sim = word_sim['similarities'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Spearman correlations (MSE) ===\n",
      "Word2Vec Skipgram: 0.09847292793281937\n",
      "Word2Vec Skipgram (NEG): 0.03240032525936758\n",
      "GloVe (Scratch): 0.0681032757240688\n",
      "GloVe (Gensim): 0.5430870624672256\n",
      "Y_true: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Compute Spearman correlations for each embedding similarity\n",
    "print(\"=== Spearman correlations (MSE) ===\")\n",
    "for name, column in embedding_columns.items():\n",
    "    if column == 'similarities':  # Skip self-correlation for Y_true\n",
    "        correlation = 1.0\n",
    "    else:\n",
    "        similarity = word_sim[column].to_numpy()\n",
    "        correlation = spearmanr(wordsim_sim, similarity).statistic\n",
    "    print(f\"{name}: {correlation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparision of models on training loss and training time  \n",
    "\n",
    "| Model                | Window Size | Training Loss | Training Time | Syntactic Accuracy | Semantic Accuracy |\n",
    "|----------------------|-------------|---------------|---------------|---------------------|--------------------|\n",
    "| Skipgram             |      2      | 9.767000      | 10min 42.5s   | 0.064%              | 0%                 |\n",
    "| Skipgram (NEG)       |      2      | 1.027903      | 10min 51.9s   | 0.064%              | 0%                 |\n",
    "| GloVe (Scratch)      |      2      | 0.672632      | 7min 15.7s    | 0%                  | 0%                 |\n",
    "| GloVe (Gensim)       | -           | -             | -             | 55.44%              | 93.87%             |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarity correlation between models’ dot product and the provided similarity metrics.\n",
    "\n",
    "| Model                | Skipgram | NEG | GloVe | GloVe (gensim) | Y_true |\n",
    "|----------------------|-------------|---------------|---------------|---------------------|--------------------|\n",
    "| MSE           | 0.09847292            | 0.03240032             | 0.06810327              | 0.54308706                    | 1                  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3. Search similar context - Web Application Development - Develop a simple website with an input box for search queries. (2 points)\n",
    "#### 1) Implement a function to compute the dot product between the input query and your corpus and retrieve the top 10 most similar context.\n",
    "#### 2) You may need to learn web frameworks like Flask or Django for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web application can be accessed locally:\n",
    "To deploy application first download app folder from github (). \n",
    "Open in VSCode and open terminal.\n",
    "In the terminal type \"python3 app.py\". My local deployment address was \"http://127.0.0.1:5000/\" however your's might be different.\n",
    "Go to browser and enter your local deployment server address to test the application.\n",
    "\n",
    "Screen shots of the working application is attached here with:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
